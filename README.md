* the app.py: extracts the audio from video
* the viii.py: get the datatset filled with videos of sign language and create a video translation
of the inputed video as an outputed the video is named :output_translation
* the rendering.py:use mediapipe teh get the pose keypoints from the video translation and create a json file
filled with the data , output:output_visualization
* animate_avatr.p: a blender script that render animation based on teh keypoints ;
* animation.py: this runs the script inside of blender: output with avatar
